{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6580b30b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "train_dir = '/opt/ml/input/data/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-organizer",
   "metadata": {},
   "source": [
    "## 1. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8e341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv_block\n",
    "# activation = relu\n",
    "# y = relu(BN(conv(x)))\n",
    "class Conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation=True, **kwargs) -> None:\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs) # kernel size = ...\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.activation:\n",
    "            return self.batchnorm(self.conv(x))\n",
    "        return self.relu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b15e70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res_block(nn.Module):\n",
    "    def __init__(self, in_channels, red_channels, out_channels, is_plain=False):\n",
    "        super(Res_block,self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.is_plain = is_plain\n",
    "        \n",
    "        if in_channels==64:\n",
    "            self.convseq = nn.Sequential(\n",
    "                Conv_block(in_channels, red_channels, kernel_size=1, padding=0),\n",
    "                Conv_block(red_channels, red_channels*2, kernel_size=3, padding=1),\n",
    "                Conv_block(red_channels*2, out_channels, activation=False, kernel_size=1, padding=0)\n",
    "            )\n",
    "            self.iden = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "        elif in_channels == out_channels:\n",
    "            self.convseq = nn.Sequential(\n",
    "                Conv_block(in_channels, red_channels, kernel_size=1, padding=0),\n",
    "                Conv_block(red_channels, red_channels*2, kernel_size=3, padding=1),\n",
    "                Conv_block(red_channels*2, out_channels, activation=False, kernel_size=1, padding=0)\n",
    "            )\n",
    "            self.iden = nn.Identity()\n",
    "        else:\n",
    "            self.convseq = nn.Sequential(\n",
    "                Conv_block(in_channels, red_channels, kernel_size=1, padding=0, stride=2),\n",
    "                Conv_block(red_channels, red_channels*2, kernel_size=3, padding=1),\n",
    "                Conv_block(red_channels*2, out_channels, activation=False, kernel_size=1, padding=0)\n",
    "                \n",
    "            )\n",
    "            self.iden = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.convseq(x)\n",
    "        if self.is_plain:\n",
    "            x = y\n",
    "        else:\n",
    "            x = y + self.iden(x)\n",
    "        x = self.relu(x)  # relu(skip connection)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acknowledged-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000, is_plain=False):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.layers = []\n",
    "        self.ksize = 3\n",
    "        \n",
    "        #L1\n",
    "        self.conv1 = Conv_block(3,32,kernel_size=self.ksize, padding=self.ksize//2, stride=(1,1))\n",
    "        self.conv2 = Conv_block(32,64,kernel_size=self.ksize, padding=self.ksize//2, stride=(2,2))\n",
    "        #Res1 \n",
    "        self.conv3_x = nn.Sequential(\n",
    "            Res_block(64, 32, 64, is_plain)\n",
    "        )\n",
    "        #L2\n",
    "        self.conv4 = Conv_block(64,128,kernel_size=self.ksize, padding=self.ksize//2, stride=(2,2))\n",
    "        #Res2  \n",
    "        self.conv5_x = nn.Sequential(\n",
    "            Res_block(128, 64, 128, is_plain),\n",
    "            Res_block(128, 64, 128, is_plain)\n",
    "        )  \n",
    "        #L3\n",
    "        self.conv6 = Conv_block(128,256,kernel_size=self.ksize, padding=self.ksize//2, stride=(2,2))\n",
    "        #Res3  \n",
    "        self.conv7_x = nn.Sequential(\n",
    "            Res_block(256, 128, 256, is_plain),\n",
    "            Res_block(256, 128, 256, is_plain),\n",
    "            Res_block(256, 128, 256, is_plain),\n",
    "            Res_block(256, 128, 256, is_plain),\n",
    "            Res_block(256, 128, 256, is_plain),\n",
    "            Res_block(256, 128, 256, is_plain),\n",
    "            Res_block(256, 128, 256, is_plain),\n",
    "            Res_block(256, 128, 256, is_plain),\n",
    "        )\n",
    "        #L4\n",
    "        self.conv8 = Conv_block(256,512,kernel_size=self.ksize, padding=self.ksize//2, stride=(2,2))\n",
    "        #Res4\n",
    "        self.conv9_x = nn.Sequential(\n",
    "            Res_block(512, 256, 512, is_plain),\n",
    "            Res_block(512, 256, 512, is_plain),\n",
    "            Res_block(512, 256, 512, is_plain),\n",
    "            Res_block(512, 256, 512, is_plain),\n",
    "            Res_block(512, 256, 512, is_plain),\n",
    "            Res_block(512, 256, 512, is_plain),\n",
    "            Res_block(512, 256, 512, is_plain),\n",
    "            Res_block(512, 256, 512, is_plain)\n",
    "        )\n",
    "        #L5\n",
    "        self.conv10 = Conv_block(512,1024,kernel_size=self.ksize, padding=self.ksize//2, stride=(2,2))\n",
    "        #Res5\n",
    "        self.conv11_x = nn.Sequential(\n",
    "            Res_block(1024, 512, 1024, is_plain),\n",
    "            Res_block(1024, 512, 1024, is_plain),\n",
    "            Res_block(1024, 512, 1024, is_plain),\n",
    "            Res_block(1024, 512, 1024, is_plain),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Dropout(),\n",
    "            # nn.Linear(64, 32),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3_x(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7_x(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9_x(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.conv11_x(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 2. Test Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, train_idx, transform):\n",
    "        self.train_idx = train_idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.train_idx[index][0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, train_idx[index][1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_idx)\n",
    "\n",
    "# class TestDataset(Dataset):\n",
    "#     def __init__(self, img_paths, transform):\n",
    "#         self.img_paths = img_paths\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         image = Image.open(self.img_paths[index])\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a781731f",
   "metadata": {},
   "source": [
    "### Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28bbea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "# 7 : 3 비율로 나눕니다.\n",
    "# submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "# submission_train = submission[:int(len(submission)*0.7)]\n",
    "# submission_test = submission[int(len(submission)*0.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80ad69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission = pd.read_csv(os.path.join(train_dir, 'train_label.csv'))\n",
    "image_dir = os.path.join(train_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "# image_paths = [os.path.join(image_dir, img_id) for img_id in submission.path]\n",
    "train_idx = [i for i in zip(submission.path,submission.label)]\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(256),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TrainDataset(train_idx, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = MyModel(num_classes=18).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87254af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력 크기 테스트\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 256, 256), device=device.type) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6d25446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def func_eval(model,data_iter,device):\n",
    "    with torch.no_grad():\n",
    "        n_total,n_correct = 0,0\n",
    "        model.eval() # evaluate (affects DropOut and BN)\n",
    "        for image, target in data_iter:\n",
    "            y_trgt = target.to(device)\n",
    "            model_pred = model(image.to(device))\n",
    "            _,y_pred = torch.max(model_pred.data,1)\n",
    "            n_correct += (y_pred==y_trgt).sum().item()\n",
    "            n_total += image.size(0)\n",
    "        val_accr = (n_correct/n_total)\n",
    "        model.train() # back to train mode \n",
    "    return val_accr\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d552387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accr = func_eval(model,loader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ac6d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accr:[0.409].\n"
     ]
    }
   ],
   "source": [
    "print (\"train_accr:[%.3f].\"%(train_accr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "341b44c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 1.7157,  1.6569,  1.5980,  ...,  1.2059,  1.1863,  1.1471],\n",
       "           [ 1.7941,  1.7745,  1.7157,  ...,  1.1667,  1.1667,  1.1275],\n",
       "           [ 1.8922,  1.8725,  1.8333,  ...,  1.1275,  1.1275,  1.1078],\n",
       "           ...,\n",
       "           [-0.6569, -0.6961, -0.7549,  ..., -0.4216, -0.4216, -0.4020],\n",
       "           [-0.6765, -0.7157, -0.7745,  ..., -0.4412, -0.4608, -0.5000],\n",
       "           [-0.6765, -0.7157, -0.7941,  ..., -0.4412, -0.4804, -0.5588]],\n",
       " \n",
       "          [[ 1.2255,  1.1667,  1.1078,  ...,  0.8725,  0.8529,  0.8137],\n",
       "           [ 1.3039,  1.2843,  1.2255,  ...,  0.8333,  0.8333,  0.7941],\n",
       "           [ 1.4020,  1.3824,  1.3431,  ...,  0.7941,  0.7941,  0.7745],\n",
       "           ...,\n",
       "           [-0.7549, -0.7941, -0.8529,  ..., -0.5000, -0.5000, -0.4804],\n",
       "           [-0.7745, -0.8137, -0.8725,  ..., -0.5196, -0.5392, -0.5784],\n",
       "           [-0.7745, -0.8137, -0.8922,  ..., -0.5196, -0.5588, -0.6373]],\n",
       " \n",
       "          [[ 0.6176,  0.5588,  0.5000,  ...,  0.2843,  0.2647,  0.2255],\n",
       "           [ 0.6961,  0.6765,  0.6176,  ...,  0.2451,  0.2451,  0.2059],\n",
       "           [ 0.7941,  0.7745,  0.7353,  ...,  0.2059,  0.2059,  0.1863],\n",
       "           ...,\n",
       "           [-0.6765, -0.7157, -0.7745,  ..., -0.4804, -0.4804, -0.4608],\n",
       "           [-0.6961, -0.7353, -0.7941,  ..., -0.5000, -0.5196, -0.5588],\n",
       "           [-0.6961, -0.7353, -0.8137,  ..., -0.5000, -0.5392, -0.6176]]]]),\n",
       " tensor([1])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d2a750a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training.\n",
      "epoch:[0] loss:[1.343] train_accr:[0.493]\n",
      "epoch:[1] loss:[0.843] train_accr:[0.550]\n",
      "epoch:[2] loss:[0.639] train_accr:[0.606]\n",
      "epoch:[3] loss:[0.486] train_accr:[0.644]\n",
      "epoch:[4] loss:[0.357] train_accr:[0.688]\n",
      "epoch:[5] loss:[0.261] train_accr:[0.566]\n",
      "epoch:[6] loss:[0.188] train_accr:[0.589]\n",
      "epoch:[7] loss:[0.140] train_accr:[0.620]\n",
      "epoch:[8] loss:[0.109] train_accr:[0.713]\n",
      "epoch:[9] loss:[0.092] train_accr:[0.680]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print (\"Start training.\")\n",
    "optm = optim.Adam(model.parameters(),lr=1e-3)\n",
    "#optm = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# model.init_param() # initialize parameters\n",
    "model.train() # to train mode \n",
    "EPOCHS,print_every = 10,1\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_val_sum = 0\n",
    "    for images, target in loader:\n",
    "        # Forward path\n",
    "        y_pred = model.forward(images.to(device))\n",
    "        loss_out = loss(y_pred,target.to(device))\n",
    "        # Update\n",
    "        optm.zero_grad()     # reset gradient \n",
    "        loss_out.backward()      # backpropagate\n",
    "        optm.step()      # optimizer update\n",
    "        loss_val_sum += loss_out\n",
    "    loss_val_avg = loss_val_sum/len(loader)\n",
    "    # Print\n",
    "    if ((epoch%print_every)==0) or (epoch==(EPOCHS-1)):\n",
    "        train_accr = func_eval(model,loader,device)\n",
    "        #test_accr = func_eval(model,test_iter,device)\n",
    "        print (\"epoch:[%d] loss:[%.3f] train_accr:[%.3f]\"%\n",
    "               (epoch,loss_val_avg,train_accr))\n",
    "        # print (\"epoch:[%d] loss:[%.3f] train_accr:[%.3f] test_accr:[%.3f].\"%\n",
    "        #        (epoch,loss_val_avg,train_accr,test_accr))\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
